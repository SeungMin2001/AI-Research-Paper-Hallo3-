# AI-Research-Paper-Hallo3-
This is a place to study the AI model 'Hallo3'.<br>
- Authors: Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, Siyu Zhu
- Submitted on 1 Dec 2024 (v1), last revised 4 Jan 2025 (this version, v3)
- 'Hallo3' Paper link -> [Research Paper](https://arxiv.org/abs/2412.00733v3)
- 'Hallo3' github -> [github](https://fudan-generative-vision.github.io/hallo3)
---
<img src='gradio.png' width=700>

# Title : Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks

## Abstract
- Existing methodologies for animating portrait images face significant, which are as follows:
  - handling Non-frontal perspectives (ë¹„ì •ë©´ì  ê´€ì  ì²˜ë¦¬)
  - rendering dynamic objects around the portrait (ì´ˆìƒí™” ì£¼ìœ„ì˜ ë™ì  ê°ì²´ ë Œë”ë§)
  - generating immersive, realistic backgrounds (ëª°ì…ê° ìˆê³  í˜„ì‹¤ì ì¸ ë°°ê²½ ìƒì„±)
- The authors stated that:
  - we design an identity reference network consisting of a causal 3D VAE combined with a stacked series of transformer layers,
    ensuring consistent facial identity across video sequences.<br>
    (ê¸°ì¡´ U-NET ë°©ì‹ì—ì„œ (3D VAE+stacked series of transformer layers) ë°©ì‹ìœ¼ë¡œ ë°”ê¿”ì„œ ì§„í–‰)
- The approach we proposed demonstrates substantial improvements.<br>
  ("substantial improvments" <-- ğŸ”¥)
---
#### Study Log
- U-NET, 3D VAE ì´ ë‘ê°œì˜ ëª¨ë¸ì— ëŒ€í•´ ì™„ì „íˆ ì´í•´í•˜ê³  êµ¬í˜„í•œê±¸ ì—¬ê¸°ì— ì €ì¥í•˜ì<br>
  - [U-NET](U-NET.md)<br>
  - [3D VAE](3D-VAE.md)
---
## Introduction
<img src='Figure1.png' width=700>
  - Existing portrait image animation have utilized facial landmarkes,3D parametric models,generative adversarial networks(GAN) and diffusion models.<br>
  ->> However
    - these methods rely on frontal and central face images.
    - making it difficult to handle side, overhead, and low-angle views.
    - they assume a static background, which limits thier ability to effectively reflect dynamic foreground and background changes.<br>
    
  - The method we used address this issue
    1. We applied a pre-trained DIT-based video generation model to portrait image animation for the first time.
    2. Identity Preservation (3D VAE+íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´)
    3. Speech Audio Conditioning (ì˜¤ë””ì˜¤ë¥¼ ì›€ì§ì„ ì œì–´ ì •ë³´ë¡œ í™œìš©)
    4. Video Extrapolation (ë¹„ë””ì˜¤ ì™¸ì‚½)
        
